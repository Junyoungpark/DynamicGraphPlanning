{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ca2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "\n",
    "from replay_buffer import anyReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f740be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a11ac",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baaa8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98716ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\", \n",
    "                       ['maxX', 'maxY', 'goal_reward', 'collision_penalty', 'distance_penalty'], \n",
    "                       defaults=[5, 5, 1., -.1, -.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66f089c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-927d573dbded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                          \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                          [0., 0.]]).to(device)\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actions = namedtuple(\"actions\", \n",
    "                    ['right', 'left', 'up', 'down', 'noop'], \n",
    "                    defaults=[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)])\n",
    "action = actions()\n",
    "a2vecmap = torch.Tensor([[1., 0.],\n",
    "                         [-1, 0.],\n",
    "                         [0., 1.],\n",
    "                         [0, -1.],\n",
    "                         [0., 0.]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a718ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connect_graph(n_drones):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones+1), r=2)\n",
    "    return to_undirected(idx.t(), num_nodes=n_drones+1)\n",
    "\n",
    "def connect_graph(n_drones, n_goal):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones), r=2)\n",
    "    edge_index = to_undirected(idx.t(), num_nodes=n_drones)\n",
    "\n",
    "    arr = torch.arange(n_drones, n_drones+n_goal)\n",
    "    for i in range(n_drones):\n",
    "        drone2goal = torch.stack([arr, i*torch.ones(n_goal, dtype=torch.int64)])\n",
    "        edge_index = torch.hstack((edge_index, drone2goal))\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, m, device='cpu'):\n",
    "        self.config = sysconfig()\n",
    "        self.ndrones = n\n",
    "        self.ngoals = m\n",
    "        \n",
    "        self.aspace = MultiDiscrete([len(action)]*self.ndrones)\n",
    "        self.sspace = MultiDiscrete([self.config.maxX, self.config.maxY, 2])\n",
    "        self.state = None\n",
    "        \n",
    "        self._device = device\n",
    "        \n",
    "    def get_distances(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[-self.ngoals:, :-1], p=1)\n",
    "        return dis.min(1).values\n",
    "    \n",
    "    def in_collision(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[:self.ndrones, :-1], p=1)\n",
    "        return torch.triu((dis == 0).float(), 1).sum().item()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY, 2])\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return (self.get_distances() == 0).all()\n",
    "        \n",
    "    def reward(self, a):\n",
    "        done = self.is_terminal().float().item()\n",
    "        return self.config.goal_reward * done + \\\n",
    "                self.config.collision_penalty * self.in_collision() * (1 - done) + \\\n",
    "                self.config.distance_penalty * env.get_distances() #.sum().item()\n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "        \n",
    "        reward = self.reward(a)\n",
    "        a = a2vecmap[a]\n",
    "        done = self.is_terminal().item()   \n",
    "    \n",
    "        self.state.x[:self.ndrones, :-1] = (self.state.x[:self.ndrones, :-1]+a).clamp(min=0, max=self.config.maxX)\n",
    "        \n",
    "        return deepcopy(self.state), deepcopy(reward), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "            \n",
    "        x = torch.Tensor(np.stack([self.sspace.sample() for _ in range(self.ndrones+self.ngoals)]))\n",
    "        # reset the state flags: +1 agent, -1 goal\n",
    "        x[:, -1] = 1\n",
    "        x[self.ndrones:, -1] = -1\n",
    "        \n",
    "        edge_index = connect_graph(self.ndrones, self.ngoals)\n",
    "        self.state = Data(x=x, edge_index=edge_index).to(device)\n",
    "        \n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self, s = None):\n",
    "        if not s:\n",
    "            s = self.state\n",
    "        g = torch_geometric.utils.to_networkx(s, to_undirected=False)\n",
    "        colors = np.array(['green']*self.ndrones+['yellow']*self.ngoals)\n",
    "        pos = {i: x[:2].cpu().numpy() for i, x in enumerate(self.state.x)}\n",
    "        nx.draw(g, pos=pos, node_color=colors)\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().seed(n)\n",
    "        self.aspace.seed(n)\n",
    "        self.sspace.seed(n)\n",
    "        \n",
    "    def to(self, device):\n",
    "        self._device = device\n",
    "        if self.state:\n",
    "            self.state = self.state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndrones = 2\n",
    "ngoals = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = droneDeliveryProbe(ndrones, ngoals, device=device)\n",
    "x = env.reset()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb122",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7aacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU, LeakyReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Sequential, GCNConv, SAGEConv\n",
    "        \n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, c_in, c_out, c_hidden=[], bounds=None, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        activation_fn = kwargs['activation'] if 'activation' in kwargs.keys() else ReLU(inplace=True)\n",
    "        \n",
    "#         self.model = Sequential('x, edge_index', [\n",
    "#             (SAGEConv(c_in, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             Linear(c_hidden, c_out),\n",
    "# #             nn.Softmax(dim=-1) # no freaking softmax\n",
    "#         ])\n",
    "        assert len(c_hidden) > 0, \"Hidden dimension can not be zero => no GCN layer.\"\n",
    "        layer_size = [c_in]+c_hidden\n",
    "        \n",
    "        self.model = Sequential('x, edge_index', [\n",
    "            x for _in, _out in zip(layer_size[:-1], layer_size[1:]) \n",
    "              for x in [(SAGEConv(_in, _out), 'x, edge_index -> x'), \n",
    "                        activation_fn]] +\n",
    "            [Linear(layer_size[-1], c_out),]\n",
    "        )\n",
    "        \n",
    "        self._device = 'cpu'\n",
    "        self._upper_bound = bounds\n",
    "        self._n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.x\n",
    "        if self._upper_bound is not None:\n",
    "            y = y.div(self._upper_bound-1)\n",
    "        return self.model(y, x.edge_index)[:self._n]\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device\n",
    "        if self._upper_bound is not None:\n",
    "            self._upper_bound = self._upper_bound.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 3, len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15950ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 32, 16], bounds=env.get_size())#, activation=LeakyReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c340cce",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = env.config.maxX + env.config.maxY - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(x):\n",
    "    probs = np.array([x[0]>0, x[0]<0, x[1]>0, x[1]<0, x[1]==x[0]==0], dtype=int)\n",
    "    return np.random.choice(action, p=probs/sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "\n",
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        idx = torch.cdist(obs.x[:self.n, :-1], obs.x[self.n:, :-1], p=1).min(1).indices\n",
    "#         pdb.set_trace()\n",
    "        dis = (obs.x[idx+self.n, :-1] - obs.x[:self.n, :-1])\n",
    "        return [pick(d.cpu()) for d in dis], {}\n",
    "    \n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=1):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=1, sim_annealing_fac=1.0, minimum=0.0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.saf = sim_annealing_fac\n",
    "        self.min = minimum\n",
    "        \n",
    "    def simulated_annealing(self):\n",
    "        self.eps = max(self.min, self.eps*self.saf)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([torch.vstack(p['rewards']).sum().item() for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return torch.hstack([torch.vstack(p['rewards']).sum(1) for p in paths]).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015a50b",
   "metadata": {},
   "source": [
    "###### For unified reward\n",
    "```python\n",
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([np.sum(p['rewards']) for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return np.hstack([p['rewards'] for p in paths]).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d81854",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_policy = sysRolloutPolicy(ndrones) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_heuristic_pt = mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_heuristic_pt)\n",
    "expected_heuristic = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb04de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 16], bounds=env.get_size())\n",
    "qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_random_pt =  mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_random_pt)\n",
    "expected_random = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_random)\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf3c7d",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "qf.to(device)\n",
    "\n",
    "target_qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1627f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.1)\n",
    "# expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.4, sim_annealing_fac=.9, minimum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.vstack(paths[0]['terminals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_cap = 10000 # 1000\n",
    "lr = 5E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58014831",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_path_collector = MdpPathCollector(env, expl_policy) \n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(replay_buffer_cap, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 25\n",
    "n_iter = 256 #128\n",
    "batch_size = 256 #128\n",
    "n_samples = 1000//max_len #replay_buffer_cap//max_len #256\n",
    "γ = 0.95\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(128, max_len, False)\n",
    "    avg_r_test.append(mean_reward_per_traj(paths))\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths) \n",
    "    \n",
    "    qf.train(True)\n",
    "    if i > 0:\n",
    "        expl_path_collector._policy.simulated_annealing()\n",
    "#         optimizer = Adam(qf.parameters(), lr=lr*(0.95**i))\n",
    "#         print(\"eps: \", expl_path_collector._policy.eps, \", lr:\", lr*(0.95**i))\n",
    "        \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "#         rewards = torch.Tensor(batch['rewards']).unsqueeze(-1).to(device)\n",
    "        rewards = torch.vstack(batch['rewards'])\n",
    "        terminals = torch.Tensor(batch['terminals']).unsqueeze(-1).to(device)\n",
    "        actions = torch.Tensor(batch['actions']).to(device)\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "                 \n",
    "        out = torch.stack(list(map(target_qf, next_obs)), axis=0)         \n",
    "        target_q_values = out.max(-1, keepdims=False).values\n",
    "        y_target = rewards + (1. - terminals) * γ * target_q_values \n",
    "        out = torch.stack(list(map(qf, obs)), axis=0)\n",
    "               \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=False)\n",
    "        qf_loss = qf_criterion(y_pred, y_target) \n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf.train(False);\n",
    "qf(x).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daa5d1",
   "metadata": {},
   "source": [
    "```python\n",
    "iter  1  -> loss:  0.0012149720391789742 , rewards: (train)  -0.06282913014001679 , (test)  -1.2653124637436122\n",
    "iter  2  -> loss:  0.0019642574864064954 , rewards: (train)  -0.054052122446591966 , (test)  -1.1287499524187297\n",
    "iter  3  -> loss:  0.0072157167173827474 , rewards: (train)  -0.03613891482382314 , (test)  -0.7096874639391899\n",
    "iter  4  -> loss:  0.009883541306408006 , rewards: (train)  -0.023849638371757464 , (test)  -0.776718711014837\n",
    "iter  5  -> loss:  0.01835099674826779 , rewards: (train)  -0.006032407800830697 , (test)  -0.7078124538529664\n",
    "iter  6  -> loss:  0.01656728388297779 , rewards: (train)  0.00302231049920465 , (test)  -0.7456249697133899\n",
    "iter  7  -> loss:  0.018635565233125817 , rewards: (train)  0.015944216541583955 , (test)  -0.49953120620921254\n",
    "iter  8  -> loss:  0.015456144439667696 , rewards: (train)  0.023251345858625427 , (test)  -0.3606249608565122\n",
    "iter  9  -> loss:  0.015704244549851865 , rewards: (train)  0.03984970464534854 , (test)  -0.5487499728333205\n",
    "iter  10  -> loss:  0.015596665951306932 , rewards: (train)  0.05474655633952352 , (test)  -0.3464061978738755\n",
    "iter  11  -> loss:  0.015487543718336383 , rewards: (train)  0.07570709744868509 , (test)  -0.29578120703808963\n",
    "iter  12  -> loss:  0.013965569749416318 , rewards: (train)  0.09062607419036794 , (test)  -0.3820312034804374\n",
    "iter  13  -> loss:  0.014347662865475286 , rewards: (train)  0.11548432186827995 , (test)  -0.3318749514874071\n",
    "iter  14  -> loss:  0.013620492038171506 , rewards: (train)  0.12642060077632777 , (test)  -0.19874995201826096\n",
    "iter  15  -> loss:  0.014426487585296854 , rewards: (train)  0.139831550695817 , (test)  -0.27906244271434844\n",
    "iter  16  -> loss:  0.014421056090213824 , rewards: (train)  0.15544373518787324 , (test)  -0.04734369437210262\n",
    "iter  17  -> loss:  0.013422946700302418 , rewards: (train)  0.1659623806772288 , (test)  -0.46171870175749063\n",
    "iter  18  -> loss:  0.01355764585241559 , rewards: (train)  0.18044022642425261 , (test)  0.1882813114207238\n",
    "iter  19  -> loss:  0.013975657806440722 , rewards: (train)  0.18655381340067834 , (test)  0.17437505396082997\n",
    "iter  20  -> loss:  0.013876441502361558 , rewards: (train)  0.1893740947416518 , (test)  -0.14859370095655322\n",
    "iter  21  -> loss:  0.014046403910469962 , rewards: (train)  0.1914555463299621 , (test)  0.030625060200691223\n",
    "iter  22  -> loss:  0.01350784004171146 , rewards: (train)  0.1877430823224131 , (test)  0.08109379815869033\n",
    "iter  23  -> loss:  0.01297064586833585 , rewards: (train)  0.19371888262685388 , (test)  0.08625005336944014\n",
    "iter  24  -> loss:  0.013545335103117395 , rewards: (train)  0.19561005668947473 , (test)  0.11109380144625902\n",
    "iter  25  -> loss:  0.013207929714553757 , rewards: (train)  0.19690705434186384 , (test)  -0.137031210353598\n",
    "\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a8f5a",
   "metadata": {},
   "source": [
    "##### notes\n",
    "1. can we increase the learning rate to 1e-2? Would that help speed the process along? loss curve it pretty flat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18470fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This eval took me \", time.time() - start_time, \" seconds. Thanks for waiting :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(qf.state_dict(), \"chkpt/baseline-2-2.pt\") #\"qf_multi_norm_heterogen1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8ce63",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random_pt]*(n_iter*n_epoch), label = \"random\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic_pt]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1  , step=n_iter), avg_r_test, label = \"avg R (test)\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('baseline-2-2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69409c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d69a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
