{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f59040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "\n",
    "from replay_buffer import anyReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308be502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fb41d",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd6181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e71b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\", \n",
    "                       ['maxX', 'maxY', 'goal_reward', 'collision_penalty', 'distance_penalty'], \n",
    "                       defaults=[5, 5, 1., -.1, -.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbec65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = namedtuple(\"actions\", \n",
    "                    ['right', 'left', 'up', 'down', 'noop'], \n",
    "                    defaults=[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)])\n",
    "action = actions()\n",
    "a2vecmap = torch.Tensor([[1., 0.],\n",
    "                         [-1, 0.],\n",
    "                         [0., 1.],\n",
    "                         [0, -1.],\n",
    "                         [0., 0.]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f778eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connect_graph(n_drones):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones+1), r=2)\n",
    "    return to_undirected(idx.t(), num_nodes=n_drones+1)\n",
    "\n",
    "def connect_graph(n_drones, n_goal):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones), r=2)\n",
    "    edge_index = to_undirected(idx.t(), num_nodes=n_drones)\n",
    "\n",
    "    arr = torch.arange(n_drones, n_drones+n_goal)\n",
    "    for i in range(n_drones):\n",
    "        drone2goal = torch.stack([arr, i*torch.ones(n_goal, dtype=torch.int64)])\n",
    "        edge_index = torch.hstack((edge_index, drone2goal))\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97902758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, m, device='cpu'):\n",
    "        self.config = sysconfig()\n",
    "        self.ndrones = n\n",
    "        self.ngoals = m\n",
    "        \n",
    "        self.aspace = MultiDiscrete([len(action)]*self.ndrones)\n",
    "        self.sspace = MultiDiscrete([self.config.maxX, self.config.maxY, 2])\n",
    "        self.state = None\n",
    "        \n",
    "        self._device = device\n",
    "        \n",
    "    def get_distances(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[-self.ngoals:, :-1], p=1)\n",
    "        return dis.min(1).values\n",
    "    \n",
    "    def in_collision(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[:self.ndrones, :-1], p=1)\n",
    "        return torch.triu((dis == 0).float(), 1).sum().item()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY, 2])\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return (self.get_distances() == 0).all()\n",
    "        \n",
    "    def reward(self, a):\n",
    "        done = self.is_terminal().float().item()\n",
    "        return self.config.goal_reward * done + \\\n",
    "                self.config.collision_penalty * self.in_collision() * (1 - done) + \\\n",
    "                self.config.distance_penalty * env.get_distances() #.sum().item()\n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "        \n",
    "        reward = self.reward(a)\n",
    "        a = a2vecmap[a]\n",
    "        done = self.is_terminal().item()   \n",
    "    \n",
    "        self.state.x[:self.ndrones, :-1] = (self.state.x[:self.ndrones, :-1]+a).clamp(min=0, max=self.config.maxX)\n",
    "        \n",
    "        return deepcopy(self.state), deepcopy(reward), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "            \n",
    "        x = torch.Tensor(np.stack([self.sspace.sample() for _ in range(self.ndrones+self.ngoals)]))\n",
    "        # reset the state flags: +1 agent, -1 goal\n",
    "        x[:, -1] = 1\n",
    "        x[self.ndrones:, -1] = -1\n",
    "        \n",
    "        edge_index = connect_graph(self.ndrones, self.ngoals)\n",
    "        self.state = Data(x=x, edge_index=edge_index).to(device)\n",
    "        \n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self, s = None):\n",
    "        if not s:\n",
    "            s = self.state\n",
    "        g = torch_geometric.utils.to_networkx(s, to_undirected=False)\n",
    "        colors = np.array(['green']*self.ndrones+['yellow']*self.ngoals)\n",
    "        pos = {i: x[:2].cpu().numpy() for i, x in enumerate(self.state.x)}\n",
    "        nx.draw(g, pos=pos, node_color=colors)\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().seed(n)\n",
    "        self.aspace.seed(n)\n",
    "        self.sspace.seed(n)\n",
    "        \n",
    "    def to(self, device):\n",
    "        self._device = device\n",
    "        if self.state:\n",
    "            self.state = self.state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87b4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndrones = 2\n",
    "ngoals = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1bf0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = droneDeliveryProbe(ndrones, ngoals, device=device)\n",
    "x = env.reset()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda27aa",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8b08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU, LeakyReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Sequential, GCNConv, SAGEConv\n",
    "        \n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, c_in, c_out, c_hidden=[], bounds=None, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        activation_fn = kwargs['activation'] if 'activation' in kwargs.keys() else ReLU(inplace=True)\n",
    "        \n",
    "#         self.model = Sequential('x, edge_index', [\n",
    "#             (SAGEConv(c_in, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             Linear(c_hidden, c_out),\n",
    "# #             nn.Softmax(dim=-1) # no freaking softmax\n",
    "#         ])\n",
    "        assert len(c_hidden) > 0, \"Hidden dimension can not be zero => no GCN layer.\"\n",
    "        layer_size = [c_in]+c_hidden\n",
    "        \n",
    "        self.model = Sequential('x, edge_index', [\n",
    "            x for _in, _out in zip(layer_size[:-1], layer_size[1:]) \n",
    "              for x in [(SAGEConv(_in, _out), 'x, edge_index -> x'), \n",
    "                        activation_fn]] +\n",
    "            [Linear(layer_size[-1], c_out),]\n",
    "        )\n",
    "        \n",
    "        self._device = 'cpu'\n",
    "        self._upper_bound = bounds\n",
    "        self._n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.x\n",
    "        if self._upper_bound is not None:\n",
    "            y = y.div(self._upper_bound-1)\n",
    "        return self.model(y, x.edge_index)[:self._n]\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device\n",
    "        if self._upper_bound is not None:\n",
    "            self._upper_bound = self._upper_bound.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7724810",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 3, len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05d8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 32, 16], bounds=env.get_size())#, activation=LeakyReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465fe37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "droneDeliveryModel(\n",
       "  (model): Sequential(\n",
       "    (0): SAGEConv(3, 32)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): SAGEConv(32, 32)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): SAGEConv(32, 16)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49338ecd",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0af7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = env.config.maxX + env.config.maxY - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "014c6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(x):\n",
    "    probs = np.array([x[0]>0, x[0]<0, x[1]>0, x[1]<0, x[1]==x[0]==0], dtype=int)\n",
    "    return np.random.choice(action, p=probs/sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb312f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "\n",
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        idx = torch.cdist(obs.x[:self.n, :-1], obs.x[self.n:, :-1], p=1).min(1).indices\n",
    "#         pdb.set_trace()\n",
    "        dis = (obs.x[idx+self.n, :-1] - obs.x[:self.n, :-1])\n",
    "        return [pick(d.cpu()) for d in dis], {}\n",
    "    \n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=1):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=1, sim_annealing_fac=1.0, minimum=0.0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.saf = sim_annealing_fac\n",
    "        self.min = minimum\n",
    "        \n",
    "    def simulated_annealing(self):\n",
    "        self.eps = max(self.min, self.eps*self.saf)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577015bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([torch.vstack(p['rewards']).sum().item() for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return torch.hstack([torch.vstack(p['rewards']).sum(1) for p in paths]).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7c200",
   "metadata": {},
   "source": [
    "###### For unified reward\n",
    "```python\n",
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([np.sum(p['rewards']) for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return np.hstack([p['rewards'] for p in paths]).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13438a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 1.7954000627994537\n",
      "Expected reward (per step): 0.4545316994190216 \n",
      "\n",
      "\n",
      " tensor([[ 4.,  1.,  1.],\n",
      "        [ 3.,  1.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 3.,  3., -1.]])\n",
      "[1, 2]\n",
      "tensor([-0.0400, -0.0200])\n",
      "False\n",
      "\n",
      " tensor([[ 3.,  1.,  1.],\n",
      "        [ 3.,  2.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 3.,  3., -1.]])\n",
      "[2, 4]\n",
      "tensor([-0.0200,  0.0000])\n",
      "False\n",
      "\n",
      " tensor([[ 3.,  2.,  1.],\n",
      "        [ 3.,  2.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 3.,  3., -1.]])\n",
      "[4, 4]\n",
      "tensor([1., 1.])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "example_policy = sysRolloutPolicy(ndrones) \n",
    "path_collector = MdpPathCollector(env, example_policy, parallelize=False)#device=='cpu')\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_heuristic_pt = mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_heuristic_pt)\n",
    "expected_heuristic = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c0ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 16], bounds=env.get_size())\n",
    "qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17dcfeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/victorialena/DynamicGraphPlanning/path_collector.py\u001b[0m(31)\u001b[0;36mcollect_new_paths\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     29 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_paths\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 31 \u001b[0;31m                \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m                \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rollout_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_path_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m                \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m/home/victorialena/DynamicGraphPlanning/path_collector.py\u001b[0m(30)\u001b[0;36mcollect_new_paths\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     28 \u001b[0;31m            \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_paths\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 30 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m                \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m                \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rollout_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_path_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-39208bee3dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margmaxDiscretePolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_collector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMdpPathCollector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_new_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# expected_random_pt =  mean_reward_per_traj(paths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Expected reward (per traj):\", expected_random_pt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DynamicGraphPlanning/path_collector.py\u001b[0m in \u001b[0;36mcollect_new_paths\u001b[0;34m(self, n_paths, max_path_length, discard_incomplete_paths, flatten)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_paths\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rollout_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_path_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36mPool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0;32m--> 119\u001b[0;31m                     context=self.get_context())\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRawValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypecode_or_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         self._worker_handler = threading.Thread(\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PoolWorker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added worker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlpyt/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 24] Too many open files"
     ]
    }
   ],
   "source": [
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy) #, parallelize=device=='cpu')\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_random_pt =  mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_random_pt)\n",
    "expected_random = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_random)\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ff71485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]},\n",
       " {'observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'actions': [array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2]),\n",
       "   array([2, 2])],\n",
       "  'rewards': [tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.]),\n",
       "   tensor([0., 0.])],\n",
       "  'next_observations': [Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6]),\n",
       "   Data(x=[4, 3], edge_index=[2, 6])],\n",
       "  'terminals': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ea60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd92cc",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "qf.to(device)\n",
    "\n",
    "target_qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82531679",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.1)\n",
    "# expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.4, sim_annealing_fac=.9, minimum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_cap = 10000 # 1000\n",
    "lr = 5E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62caff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_path_collector = MdpPathCollector(env, expl_policy) \n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(replay_buffer_cap, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 25\n",
    "n_iter = 256 #128\n",
    "batch_size = 256 #128\n",
    "n_samples = 1000//max_len #replay_buffer_cap//max_len #256\n",
    "γ = 0.95\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(128, max_len, False)\n",
    "    avg_r_test.append(mean_reward_per_traj(paths))\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths) \n",
    "    \n",
    "    qf.train(True)\n",
    "    if i > 0:\n",
    "        expl_path_collector._policy.simulated_annealing()\n",
    "#         optimizer = Adam(qf.parameters(), lr=lr*(0.95**i))\n",
    "#         print(\"eps: \", expl_path_collector._policy.eps, \", lr:\", lr*(0.95**i))\n",
    "        \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "#         rewards = torch.Tensor(batch['rewards']).unsqueeze(-1).to(device)\n",
    "        rewards = torch.vstack(batch['rewards'])\n",
    "        terminals = torch.Tensor(batch['terminals']).unsqueeze(-1).to(device)\n",
    "        actions = torch.Tensor(batch['actions']).to(device)\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "                 \n",
    "        out = torch.stack(list(map(target_qf, next_obs)), axis=0)         \n",
    "        target_q_values = out.max(-1, keepdims=False).values\n",
    "        y_target = rewards + (1. - terminals) * γ * target_q_values \n",
    "        out = torch.stack(list(map(qf, obs)), axis=0)\n",
    "               \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=False)\n",
    "        qf_loss = qf_criterion(y_pred, y_target) \n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6960758",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf.train(False);\n",
    "qf(x).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f79022",
   "metadata": {},
   "source": [
    "```python\n",
    "iter  1  -> loss:  0.0012149720391789742 , rewards: (train)  -0.06282913014001679 , (test)  -1.2653124637436122\n",
    "iter  2  -> loss:  0.0019642574864064954 , rewards: (train)  -0.054052122446591966 , (test)  -1.1287499524187297\n",
    "iter  3  -> loss:  0.0072157167173827474 , rewards: (train)  -0.03613891482382314 , (test)  -0.7096874639391899\n",
    "iter  4  -> loss:  0.009883541306408006 , rewards: (train)  -0.023849638371757464 , (test)  -0.776718711014837\n",
    "iter  5  -> loss:  0.01835099674826779 , rewards: (train)  -0.006032407800830697 , (test)  -0.7078124538529664\n",
    "iter  6  -> loss:  0.01656728388297779 , rewards: (train)  0.00302231049920465 , (test)  -0.7456249697133899\n",
    "iter  7  -> loss:  0.018635565233125817 , rewards: (train)  0.015944216541583955 , (test)  -0.49953120620921254\n",
    "iter  8  -> loss:  0.015456144439667696 , rewards: (train)  0.023251345858625427 , (test)  -0.3606249608565122\n",
    "iter  9  -> loss:  0.015704244549851865 , rewards: (train)  0.03984970464534854 , (test)  -0.5487499728333205\n",
    "iter  10  -> loss:  0.015596665951306932 , rewards: (train)  0.05474655633952352 , (test)  -0.3464061978738755\n",
    "iter  11  -> loss:  0.015487543718336383 , rewards: (train)  0.07570709744868509 , (test)  -0.29578120703808963\n",
    "iter  12  -> loss:  0.013965569749416318 , rewards: (train)  0.09062607419036794 , (test)  -0.3820312034804374\n",
    "iter  13  -> loss:  0.014347662865475286 , rewards: (train)  0.11548432186827995 , (test)  -0.3318749514874071\n",
    "iter  14  -> loss:  0.013620492038171506 , rewards: (train)  0.12642060077632777 , (test)  -0.19874995201826096\n",
    "iter  15  -> loss:  0.014426487585296854 , rewards: (train)  0.139831550695817 , (test)  -0.27906244271434844\n",
    "iter  16  -> loss:  0.014421056090213824 , rewards: (train)  0.15544373518787324 , (test)  -0.04734369437210262\n",
    "iter  17  -> loss:  0.013422946700302418 , rewards: (train)  0.1659623806772288 , (test)  -0.46171870175749063\n",
    "iter  18  -> loss:  0.01355764585241559 , rewards: (train)  0.18044022642425261 , (test)  0.1882813114207238\n",
    "iter  19  -> loss:  0.013975657806440722 , rewards: (train)  0.18655381340067834 , (test)  0.17437505396082997\n",
    "iter  20  -> loss:  0.013876441502361558 , rewards: (train)  0.1893740947416518 , (test)  -0.14859370095655322\n",
    "iter  21  -> loss:  0.014046403910469962 , rewards: (train)  0.1914555463299621 , (test)  0.030625060200691223\n",
    "iter  22  -> loss:  0.01350784004171146 , rewards: (train)  0.1877430823224131 , (test)  0.08109379815869033\n",
    "iter  23  -> loss:  0.01297064586833585 , rewards: (train)  0.19371888262685388 , (test)  0.08625005336944014\n",
    "iter  24  -> loss:  0.013545335103117395 , rewards: (train)  0.19561005668947473 , (test)  0.11109380144625902\n",
    "iter  25  -> loss:  0.013207929714553757 , rewards: (train)  0.19690705434186384 , (test)  -0.137031210353598\n",
    "\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46892239",
   "metadata": {},
   "source": [
    "##### notes\n",
    "1. can we increase the learning rate to 1e-2? Would that help speed the process along? loss curve it pretty flat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This eval took me \", time.time() - start_time, \" seconds. Thanks for waiting :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(qf.state_dict(), \"chkpt/baseline-2-2.pt\") #\"qf_multi_norm_heterogen1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6abfdc",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69792698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b43827",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random_pt]*(n_iter*n_epoch), label = \"random\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic_pt]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1  , step=n_iter), avg_r_test, label = \"avg R (test)\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('baseline-2-2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a29b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193a5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
