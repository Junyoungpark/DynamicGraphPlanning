{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1389499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "\n",
    "from any_replay_buffer import anyReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e265219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e295a4",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f4f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec20c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\", \n",
    "                       ['maxX', 'maxY', 'goal_reward', 'collision_penalty', 'distance_penalty'], \n",
    "                       defaults=[5, 5, 1., -.1, -.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4938bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = namedtuple(\"actions\", \n",
    "                    ['right', 'left', 'up', 'down', 'noop'], \n",
    "                    defaults=[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)])\n",
    "action = actions()\n",
    "a2vecmap = torch.Tensor([[1., 0.],\n",
    "                         [-1, 0.],\n",
    "                         [0., 1.],\n",
    "                         [0, -1.],\n",
    "                         [0., 0.]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb24fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connect_graph(n_drones):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones+1), r=2)\n",
    "    return to_undirected(idx.t(), num_nodes=n_drones+1)\n",
    "\n",
    "def connect_graph(n_drones, n_goal):\n",
    "    \"\"\" Connect the graph s.t. all drones are interconnected and each goal connects to all drones. \"\"\"\n",
    "    \n",
    "    idx = torch.combinations(torch.arange(n_drones), r=2)\n",
    "    edge_index = to_undirected(idx.t(), num_nodes=n_drones)\n",
    "\n",
    "    arr = torch.arange(n_drones, n_drones+n_goal)\n",
    "    for i in range(n_drones):\n",
    "        drone2goal = torch.stack([arr, i*torch.ones(n_goal, dtype=torch.int64)])\n",
    "        edge_index = torch.hstack((edge_index, drone2goal))\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b634980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, m, device='cpu'):\n",
    "        self.config = sysconfig()\n",
    "        self.ndrones = n\n",
    "        self.ngoals = m\n",
    "        \n",
    "        self.aspace = MultiDiscrete([len(action)]*self.ndrones)\n",
    "        self.sspace = MultiDiscrete([self.config.maxX, self.config.maxY, 2])\n",
    "        self.state = None\n",
    "        \n",
    "        self._device = device\n",
    "        \n",
    "    def get_distances(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[-self.ngoals:, :-1], p=1)\n",
    "        return dis.min(1).values\n",
    "    \n",
    "    def in_collision(self):\n",
    "        dis = torch.cdist(self.state.x[:self.ndrones, :-1], self.state.x[:self.ndrones, :-1], p=1)\n",
    "        return torch.triu((dis == 0).float(), 1).sum().item()\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY, 2])\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return (self.get_distances() == 0).all()\n",
    "        \n",
    "    def reward(self, a):\n",
    "        done = self.is_terminal().float().item()\n",
    "        return self.config.goal_reward * done + \\\n",
    "                self.config.collision_penalty * self.in_collision() * (1 - done) + \\\n",
    "                self.config.distance_penalty * env.get_distances() #.sum().item()\n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "        \n",
    "        reward = self.reward(a)\n",
    "        a = a2vecmap[a]\n",
    "        done = self.is_terminal().item()   \n",
    "    \n",
    "        self.state.x[:self.ndrones, :-1] = (self.state.x[:self.ndrones, :-1]+a).clamp(min=0, max=self.config.maxX)\n",
    "        \n",
    "        return deepcopy(self.state), deepcopy(reward), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "            \n",
    "        x = torch.Tensor(np.stack([self.sspace.sample() for _ in range(self.ndrones+self.ngoals)]))\n",
    "        # reset the state flags: +1 agent, -1 goal\n",
    "        x[:, -1] = 1\n",
    "        x[self.ndrones:, -1] = -1\n",
    "        \n",
    "        edge_index = connect_graph(self.ndrones, self.ngoals)\n",
    "        self.state = Data(x=x, edge_index=edge_index).to(device)\n",
    "        \n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self, s = None):\n",
    "        if not s:\n",
    "            s = self.state\n",
    "        g = torch_geometric.utils.to_networkx(s, to_undirected=False)\n",
    "        colors = np.array(['green']*self.ndrones+['yellow']*self.ngoals)\n",
    "        pos = {i: x[:2].cpu().numpy() for i, x in enumerate(self.state.x)}\n",
    "        nx.draw(g, pos=pos, node_color=colors)\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().seed(n)\n",
    "        self.aspace.seed(n)\n",
    "        self.sspace.seed(n)\n",
    "        \n",
    "    def to(self, device):\n",
    "        self._device = device\n",
    "        if self.state:\n",
    "            self.state = self.state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96fbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndrones = 2\n",
    "ngoals = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e872018c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = droneDeliveryProbe(ndrones, ngoals, device=device)\n",
    "x = env.reset()\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c6a9d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a81b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU, LeakyReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Sequential, GCNConv, SAGEConv\n",
    "        \n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, c_in, c_out, c_hidden=[], bounds=None, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        activation_fn = kwargs['activation'] if 'activation' in kwargs.keys() else ReLU(inplace=True)\n",
    "        \n",
    "#         self.model = Sequential('x, edge_index', [\n",
    "#             (SAGEConv(c_in, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             (SAGEConv(c_hidden, c_hidden), 'x, edge_index -> x'),\n",
    "#             activation_fn,\n",
    "#             Linear(c_hidden, c_out),\n",
    "# #             nn.Softmax(dim=-1) # no freaking softmax\n",
    "#         ])\n",
    "        assert len(c_hidden) > 0, \"Hidden dimension can not be zero => no GCN layer.\"\n",
    "        layer_size = [c_in]+c_hidden\n",
    "        \n",
    "        self.model = Sequential('x, edge_index', [\n",
    "            x for _in, _out in zip(layer_size[:-1], layer_size[1:]) \n",
    "              for x in [(SAGEConv(_in, _out), 'x, edge_index -> x'), \n",
    "                        activation_fn]] +\n",
    "            [Linear(layer_size[-1], c_out),]\n",
    "        )\n",
    "        \n",
    "        self._device = 'cpu'\n",
    "        self._upper_bound = bounds\n",
    "        self._n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.x\n",
    "        if self._upper_bound is not None:\n",
    "            y = y.div(self._upper_bound-1)\n",
    "        return self.model(y, x.edge_index)[:self._n]\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device\n",
    "        if self._upper_bound is not None:\n",
    "            self._upper_bound = self._upper_bound.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea18c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 3, len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4abfb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 32, 16], bounds=env.get_size())#, activation=LeakyReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a08a83c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "droneDeliveryModel(\n",
       "  (model): Sequential(\n",
       "    (0): SAGEConv(3, 32)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): SAGEConv(32, 32)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): SAGEConv(32, 16)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c488211",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f260d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = env.config.maxX + env.config.maxY - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24218056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick(x):\n",
    "    probs = np.array([x[0]>0, x[0]<0, x[1]>0, x[1]<0, x[1]==x[0]==0], dtype=int)\n",
    "    return np.random.choice(action, p=probs/sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21b6824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "\n",
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        idx = torch.cdist(obs.x[:self.n, :-1], obs.x[self.n:, :-1], p=1).min(1).indices\n",
    "#         pdb.set_trace()\n",
    "        dis = (obs.x[idx+self.n, :-1] - obs.x[:self.n, :-1])\n",
    "        return [pick(d.cpu()) for d in dis], {}\n",
    "    \n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=1):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=1, sim_annealing_fac=1.0, minimum=0.0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.saf = sim_annealing_fac\n",
    "        self.min = minimum\n",
    "        \n",
    "    def simulated_annealing(self):\n",
    "        self.eps = max(self.min, self.eps*self.saf)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94c9b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([torch.vstack(p['rewards']).sum().item() for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return torch.hstack([torch.vstack(p['rewards']).sum(1) for p in paths]).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c452c",
   "metadata": {},
   "source": [
    "###### For unified reward\n",
    "```python\n",
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([np.sum(p['rewards']) for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return np.hstack([p['rewards'] for p in paths]).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "396d656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 1.7770000457763673\n",
      "Expected reward (per step): 0.4302663803100586 \n",
      "\n",
      "\n",
      " tensor([[ 4.,  2.,  1.],\n",
      "        [ 2.,  1.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 0.,  1., -1.]], device='cuda:0')\n",
      "[1, 2]\n",
      "tensor([-0.0200, -0.0400], device='cuda:0')\n",
      "False\n",
      "\n",
      " tensor([[ 3.,  2.,  1.],\n",
      "        [ 2.,  2.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 0.,  1., -1.]], device='cuda:0')\n",
      "[4, 0]\n",
      "tensor([ 0.0000, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " tensor([[ 3.,  2.,  1.],\n",
      "        [ 3.,  2.,  1.],\n",
      "        [ 3.,  2., -1.],\n",
      "        [ 0.,  1., -1.]], device='cuda:0')\n",
      "[4, 4]\n",
      "tensor([1., 1.], device='cuda:0')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "example_policy = sysRolloutPolicy(ndrones) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_heuristic_pt = mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_heuristic_pt)\n",
    "expected_heuristic = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d621a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32, 16], bounds=env.get_size())\n",
    "qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1eb8d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): -1.1467999750375748\n",
      "Expected reward (per step): -0.1294356733560562\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0600, -0.0400], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0400, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0200,  0.0000], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([ 0.0000, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0200, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0400, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0400, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0400, -0.0200], device='cuda:0')\n",
      "False\n",
      "\n",
      " Data(x=[4, 3], edge_index=[2, 6])\n",
      "[2 2]\n",
      "tensor([-0.0400, -0.0200], device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_random_pt =  mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_random_pt)\n",
    "expected_random = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_random)\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print('\\n', s)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fb00a",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720eaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "qf.to(device)\n",
    "\n",
    "target_qf = droneDeliveryModel(ndrones, in_channels, out_channels, [32]*3, bounds=env.get_size())\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.1)\n",
    "# expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=.4, sim_annealing_fac=.9, minimum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b7d343b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f76068a0bbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got bool"
     ]
    }
   ],
   "source": [
    "torch.vstack(paths[0]['terminals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9c0bf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observations', 'actions', 'rewards', 'next_observations', 'terminals', 'dones', 'agent_infos', 'env_infos', 'full_observations', 'full_next_observations'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_cap = 10000 # 1000\n",
    "lr = 5E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24b98b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df6b5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_path_collector = MdpPathCollector(env, expl_policy) \n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(replay_buffer_cap, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 25\n",
    "n_iter = 256 #128\n",
    "batch_size = 256 #128\n",
    "n_samples = 1000//max_len #replay_buffer_cap//max_len #256\n",
    "γ = 0.95\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(128, max_len, False)\n",
    "    avg_r_test.append(mean_reward_per_traj(paths))\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths) \n",
    "    \n",
    "    qf.train(True)\n",
    "    if i > 0:\n",
    "        expl_path_collector._policy.simulated_annealing()\n",
    "#         optimizer = Adam(qf.parameters(), lr=lr*(0.95**i))\n",
    "#         print(\"eps: \", expl_path_collector._policy.eps, \", lr:\", lr*(0.95**i))\n",
    "        \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "#         rewards = torch.Tensor(batch['rewards']).unsqueeze(-1).to(device)\n",
    "        rewards = torch.vstack(batch['rewards'])\n",
    "        terminals = torch.Tensor(batch['terminals']).unsqueeze(-1).to(device)\n",
    "        actions = torch.Tensor(batch['actions']).to(device)\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "                 \n",
    "        out = torch.stack(list(map(target_qf, next_obs)), axis=0)         \n",
    "        target_q_values = out.max(-1, keepdims=False).values\n",
    "        y_target = rewards + (1. - terminals) * γ * target_q_values \n",
    "        out = torch.stack(list(map(qf, obs)), axis=0)\n",
    "               \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=False)\n",
    "        qf_loss = qf_criterion(y_pred, y_target) \n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ae30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf.train(False);\n",
    "qf(x).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce7a13",
   "metadata": {},
   "source": [
    "```python\n",
    "iter  1  -> loss:  0.0012149720391789742 , rewards: (train)  -0.06282913014001679 , (test)  -1.2653124637436122\n",
    "iter  2  -> loss:  0.0019642574864064954 , rewards: (train)  -0.054052122446591966 , (test)  -1.1287499524187297\n",
    "iter  3  -> loss:  0.0072157167173827474 , rewards: (train)  -0.03613891482382314 , (test)  -0.7096874639391899\n",
    "iter  4  -> loss:  0.009883541306408006 , rewards: (train)  -0.023849638371757464 , (test)  -0.776718711014837\n",
    "iter  5  -> loss:  0.01835099674826779 , rewards: (train)  -0.006032407800830697 , (test)  -0.7078124538529664\n",
    "iter  6  -> loss:  0.01656728388297779 , rewards: (train)  0.00302231049920465 , (test)  -0.7456249697133899\n",
    "iter  7  -> loss:  0.018635565233125817 , rewards: (train)  0.015944216541583955 , (test)  -0.49953120620921254\n",
    "iter  8  -> loss:  0.015456144439667696 , rewards: (train)  0.023251345858625427 , (test)  -0.3606249608565122\n",
    "iter  9  -> loss:  0.015704244549851865 , rewards: (train)  0.03984970464534854 , (test)  -0.5487499728333205\n",
    "iter  10  -> loss:  0.015596665951306932 , rewards: (train)  0.05474655633952352 , (test)  -0.3464061978738755\n",
    "iter  11  -> loss:  0.015487543718336383 , rewards: (train)  0.07570709744868509 , (test)  -0.29578120703808963\n",
    "iter  12  -> loss:  0.013965569749416318 , rewards: (train)  0.09062607419036794 , (test)  -0.3820312034804374\n",
    "iter  13  -> loss:  0.014347662865475286 , rewards: (train)  0.11548432186827995 , (test)  -0.3318749514874071\n",
    "iter  14  -> loss:  0.013620492038171506 , rewards: (train)  0.12642060077632777 , (test)  -0.19874995201826096\n",
    "iter  15  -> loss:  0.014426487585296854 , rewards: (train)  0.139831550695817 , (test)  -0.27906244271434844\n",
    "iter  16  -> loss:  0.014421056090213824 , rewards: (train)  0.15544373518787324 , (test)  -0.04734369437210262\n",
    "iter  17  -> loss:  0.013422946700302418 , rewards: (train)  0.1659623806772288 , (test)  -0.46171870175749063\n",
    "iter  18  -> loss:  0.01355764585241559 , rewards: (train)  0.18044022642425261 , (test)  0.1882813114207238\n",
    "iter  19  -> loss:  0.013975657806440722 , rewards: (train)  0.18655381340067834 , (test)  0.17437505396082997\n",
    "iter  20  -> loss:  0.013876441502361558 , rewards: (train)  0.1893740947416518 , (test)  -0.14859370095655322\n",
    "iter  21  -> loss:  0.014046403910469962 , rewards: (train)  0.1914555463299621 , (test)  0.030625060200691223\n",
    "iter  22  -> loss:  0.01350784004171146 , rewards: (train)  0.1877430823224131 , (test)  0.08109379815869033\n",
    "iter  23  -> loss:  0.01297064586833585 , rewards: (train)  0.19371888262685388 , (test)  0.08625005336944014\n",
    "iter  24  -> loss:  0.013545335103117395 , rewards: (train)  0.19561005668947473 , (test)  0.11109380144625902\n",
    "iter  25  -> loss:  0.013207929714553757 , rewards: (train)  0.19690705434186384 , (test)  -0.137031210353598\n",
    "\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e09c5",
   "metadata": {},
   "source": [
    "##### notes\n",
    "1. can we increase the learning rate to 1e-2? Would that help speed the process along? loss curve it pretty flat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This eval took me \", time.time() - start_time, \" seconds. Thanks for waiting :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(qf.state_dict(), \"chkpt/baseline-2-2.pt\") #\"qf_multi_norm_heterogen1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86a287",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72229d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eae426",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random_pt]*(n_iter*n_epoch), label = \"random\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic_pt]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1  , step=n_iter), avg_r_test, label = \"avg R (test)\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('baseline-2-2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662733f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
