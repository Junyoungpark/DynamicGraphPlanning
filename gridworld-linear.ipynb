{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5dcebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "\n",
    "from any_replay_buffer import anyReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed8c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76172f26",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d33c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d2aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\",\n",
    "                       ['maxX', 'maxY', 'goal_reward'], \n",
    "                       defaults=[2, 2, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0c75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = namedtuple(\"actions\", \n",
    "                    ['right', 'left', 'up', 'down'], \n",
    "                    defaults=[np.int64(0), np.int64(1), np.int64(2), np.int64(3)])\n",
    "action = actions()\n",
    "a2vecmap = torch.Tensor([[1., 0.],\n",
    "                         [-1, 0.],\n",
    "                         [0., 1.],\n",
    "                         [0, -1.]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a323a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device = 'cpu'):\n",
    "        self.config = sysconfig()\n",
    "        \n",
    "        self.aspace = MultiDiscrete(len(action))\n",
    "        self.sspace = MultiDiscrete([self.config.maxX, self.config.maxY])\n",
    "        self.state = None\n",
    "        \n",
    "        self._device = device\n",
    "        \n",
    "    def get_distances(self):\n",
    "        return (self.state[2:]-self.state[:2]).norm(p=1)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY])\n",
    "        \n",
    "    def reward(self, a):\n",
    "        return (self.state[:2] == self.state[2:]).all().float().item()\n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "        \n",
    "        reward = self.reward(a)\n",
    "        a = a2vecmap[a]\n",
    "        done = (self.state[:2] == self.state[2:]).all().item()\n",
    "    \n",
    "        self.state[:2] = (self.state[:2]+a).clamp(min=0, max=1)\n",
    "        \n",
    "        return deepcopy(self.state), deepcopy(reward), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "          \n",
    "        self.state = torch.Tensor([self.sspace.sample() for _ in range(2)]).flatten().to(device)\n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().seed(n)\n",
    "        self.aspace.seed(n)\n",
    "        self.sspace.seed(n)\n",
    "        \n",
    "    def to(self, device):\n",
    "        self._device = device\n",
    "        if self.state:\n",
    "            self.state = self.state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cfa8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = droneDeliveryProbe(device)\n",
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9719f1c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1995b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "        \n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, c_hidden=64, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('lin1', Linear(c_in, c_hidden)),\n",
    "            ('relu1', ReLU()),\n",
    "            ('lin2', Linear(c_hidden, c_out)),\n",
    "#             nn.Softmax(dim=-1) # no freaking softmax\n",
    "        ]))\n",
    "\n",
    "        self._device = 'cpu'  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c152e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 4, len(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddee9c",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1142c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = env.config.maxX + env.config.maxY - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1219bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "\n",
    "pick = lambda x: np.random.choice(action, p=x/sum(x) if sum(x) != 0 else None)\n",
    "\n",
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        dis = (obs[2:] - obs[:2]).cpu().numpy()\n",
    "        p = np.array([dis[0] > 0, dis[0] < 0, dis[1] > 0, dis[1] < 0], dtype=int)\n",
    "        return pick(p), {}\n",
    "    \n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad497a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward_per_traj(paths):\n",
    "    return np.mean([np.sum(p['rewards']) for p in paths])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return np.hstack([p['rewards'] for p in paths]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf2693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 1.0\n",
      "Expected reward (per step): 0.49504950495049505 \n",
      "\n",
      "tensor([0., 0., 1., 1.], device='cuda:0')\n",
      "0\n",
      "0.0\n",
      "False\n",
      "tensor([1., 0., 1., 1.], device='cuda:0')\n",
      "2\n",
      "0.0\n",
      "False\n",
      "tensor([1., 1., 1., 1.], device='cuda:0')\n",
      "2\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "example_policy = sysRolloutPolicy() \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_heuristic = mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_heuristic)\n",
    "expected_heuristic = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print(s)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22fcdc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 0.34\n",
      "Expected reward (per step): 0.14166666666666666 \n",
      "\n",
      "tensor([0., 1., 0., 0.], device='cuda:0')\n",
      "0\n",
      "0.0\n",
      "False\n",
      "tensor([1., 1., 0., 0.], device='cuda:0')\n",
      "0\n",
      "0.0\n",
      "False\n",
      "tensor([1., 1., 0., 0.], device='cuda:0')\n",
      "0\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "qf = droneDeliveryModel(in_channels, out_channels, 8)\n",
    "qf.to(device)\n",
    "\n",
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_random =  mean_reward_per_traj(paths)\n",
    "print(\"Expected reward (per traj):\", expected_random)\n",
    "expected_random = mean_reward(paths)\n",
    "print(\"Expected reward (per step):\", expected_random, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print(s)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bf580",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2683c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1186948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(in_channels, out_channels, 16)\n",
    "qf.to(device)\n",
    "\n",
    "target_qf = droneDeliveryModel(in_channels, out_channels, 16)\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dce4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5025166",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_path_collector = MdpPathCollector(env, expl_policy) \n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(128, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=5E-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c72e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1  -> loss:  0.1511211445555091 , rewards: (train)  0.401796875 , (test)  0.4375\n",
      "iter  2  -> loss:  0.019223500452935697 , rewards: (train)  0.303984375 , (test)  0.2109375\n",
      "iter  3  -> loss:  0.004559190467698499 , rewards: (train)  0.434375 , (test)  0.453125\n",
      "iter  4  -> loss:  0.005707259718328714 , rewards: (train)  0.473515625 , (test)  0.546875\n",
      "iter  5  -> loss:  0.001630101787741296 , rewards: (train)  0.606640625 , (test)  0.890625\n",
      "iter  6  -> loss:  0.0004854440772760427 , rewards: (train)  0.609765625 , (test)  1.0\n",
      "iter  7  -> loss:  0.0004920277573546627 , rewards: (train)  0.579765625 , (test)  0.65625\n",
      "iter  8  -> loss:  0.00016274868827167665 , rewards: (train)  0.565078125 , (test)  1.0\n",
      "iter  9  -> loss:  0.0003416326496517286 , rewards: (train)  0.4934375 , (test)  0.8828125\n",
      "iter  10  -> loss:  0.00014817242554272526 , rewards: (train)  0.643046875 , (test)  1.0\n",
      "iter  11  -> loss:  8.460754726911545e-05 , rewards: (train)  0.65734375 , (test)  1.0\n",
      "iter  12  -> loss:  9.61910141268163e-05 , rewards: (train)  0.605 , (test)  1.0\n",
      "iter  13  -> loss:  9.228643708411255e-05 , rewards: (train)  0.584453125 , (test)  1.0\n",
      "iter  14  -> loss:  4.457269795238972e-05 , rewards: (train)  0.694609375 , (test)  1.0\n",
      "iter  15  -> loss:  0.0001436984210886294 , rewards: (train)  0.52265625 , (test)  1.0\n",
      "iter  16  -> loss:  0.0001075572331683361 , rewards: (train)  0.607265625 , (test)  0.9765625\n",
      "iter  17  -> loss:  0.00010897502641455504 , rewards: (train)  0.6134375 , (test)  1.0\n",
      "iter  18  -> loss:  8.052229617533157e-05 , rewards: (train)  0.581875 , (test)  1.0\n",
      "iter  19  -> loss:  7.637303864612477e-05 , rewards: (train)  0.646484375 , (test)  1.0\n",
      "iter  20  -> loss:  6.755583563062828e-05 , rewards: (train)  0.574453125 , (test)  1.0\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "n_iter = 100\n",
    "batch_size = 128\n",
    "n_samples = 256\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(batch_size, max_len, False)\n",
    "    avg_r_test.append(mean_reward_per_traj(paths))\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    qf.train(True)    \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "        rewards = torch.Tensor(batch['rewards']).unsqueeze(-1).to(device)\n",
    "        terminals = torch.Tensor(batch['terminals']).unsqueeze(-1).to(device)\n",
    "        actions = torch.Tensor(batch['actions']).to(device)\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "                \n",
    "        X = torch.stack(next_obs)\n",
    "        out = target_qf(X)\n",
    "        \n",
    "        target_q_values = out.max(-1, keepdim=True).values\n",
    "        y_target = rewards + (1. - terminals) * 0.90 * target_q_values\n",
    "        \n",
    "        X = torch.stack(obs)\n",
    "        out = qf(X)\n",
    "               \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=True)\n",
    "        qf_loss = qf_criterion(y_pred, y_target)\n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3d5cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This eval took me  9.360820770263672  seconds. Thanks for waiting :)\n"
     ]
    }
   ],
   "source": [
    "print(\"This eval took me \", time.time() - start_time, \" seconds. Thanks for waiting :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29c643fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf.train(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2ec1539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9875, 0.9990, 1.0198, 1.0001], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qf(torch.Tensor([0, 0, 0, 0]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4528568",
   "metadata": {},
   "source": [
    "```python \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random]*(n_iter*n_epoch), label = \"random\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "# plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1, step=n_iter), avg_r_test, label = \"avg R (test)\") \n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('training_log.png', dpi=300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9cc27",
   "metadata": {},
   "source": [
    "#### options ot make it better\n",
    "1. curriculum learning\n",
    "2. higher eps\n",
    "3. normalize input\n",
    "4. better exploration strategy\n",
    "5. one hot encode input (does not scale well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bdb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
