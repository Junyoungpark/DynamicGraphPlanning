{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717de9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "\n",
    "from any_replay_buffer import anyReplayBuffer\n",
    "# from policies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72b797",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d35b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\", \n",
    "                       ['arange', 'maxX', 'maxY', 'goal_reward'], \n",
    "                       defaults=[1., 3., 3., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86be8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = namedtuple(\"actions\", \n",
    "                    ['up', 'down', 'right', 'left'], \n",
    "                    defaults=[np.int64(0), np.int64(1), np.int64(2), np.int64(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adce8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6d5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2vecmap = {i: v for i, v in enumerate(torch.Tensor([[0., 1.],\n",
    "                                                     [0, -1.],\n",
    "                                                     [1., 0.], \n",
    "                                                     [-1., 0]]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ee8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = sysconfig()\n",
    "        \n",
    "        self.aspace = MultiDiscrete(len(action))\n",
    "        self.sspace = Box(low=np.zeros((2,2)), \n",
    "                          high=np.hstack([np.ones((2,1))*self.config.maxX, np.ones((2,1))*self.config.maxY]),\n",
    "                          dtype=np.float32)\n",
    "        self.state = None\n",
    "        \n",
    "    def get_distances(self):\n",
    "        return (self.state.x[1]-self.state.x[0]).norm(p=1, keepdim=True)#, dim=1)\n",
    "#         return torch.cdist(X, S[self.n_drones:, :2], p=1)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY])\n",
    "        \n",
    "    def reward(self, a):\n",
    "        dis = self.get_distances()        \n",
    "        return (dis==0).sum() * self.config.goal_reward \n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "                \n",
    "        a = a2vecmap[a]\n",
    "        reward = self.reward(a)\n",
    "        done = all(self.get_distances() == 0)\n",
    "        \n",
    "        self.state.x[0] = (self.state.x[0]+a).clamp(min=torch.zeros(2), max=self.get_size()-1)        \n",
    "        return deepcopy(self.state), deepcopy(reward.item()), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "            \n",
    "        x = torch.Tensor(self.sspace.sample())\n",
    "        x -= x % self.config.arange\n",
    "        \n",
    "        edge_index = torch.eye(2, dtype=torch.int64)\n",
    "        self.state = Data(x=x, edge_index=edge_index)\n",
    "        \n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().reset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa6cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2351130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = droneDeliveryProbe()\n",
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a6365",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9b0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd14d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Sequential, GCNConv, SAGEConv\n",
    "\n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, c_hidden=64, maxXY = None, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = Sequential('x, edge_index', [\n",
    "            (SAGEConv(c_in, c_hidden), 'x, edge_index -> x'),\n",
    "            ReLU(inplace=True),\n",
    "#             (SAGEConv(c_hidden c_hidden), 'x, edge_index -> x'),\n",
    "#             ReLU(inplace=True),\n",
    "            Linear(c_hidden, c_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ])\n",
    "\n",
    "        self._device = 'cpu'\n",
    "        self.maxXY = maxXY        \n",
    "\n",
    "    def forward(self, x):\n",
    "        X = x.x\n",
    "        if self.maxXY != None:\n",
    "            X = X.div(self.maxXY) - 0.5\n",
    "        return self.model(X.to(self._device), x.edge_index.to(self._device))[0]\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eed92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 2, len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c174053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(in_channels, out_channels, 8, maxXY = env.get_size())\n",
    "qf.to(device)\n",
    "\n",
    "target_qf = droneDeliveryModel(in_channels, out_channels, 8, maxXY = env.get_size())\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c0527",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88c044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f082c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "# from policies import *\n",
    "\n",
    "pick = lambda x: np.random.choice(action, p=x/sum(x) if sum(x) != 0 else None)\n",
    "\n",
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        dis = (obs.x[1] - obs.x[0]).numpy()\n",
    "        p = [dis[1] > 0, dis[1] < 0, dis[0] > 0, dis[0] < 0]\n",
    "        return pick(p), {}\n",
    "    \n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919ec06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 0.29\n",
      "Expected reward (per step): 0.03830911492734478 \n",
      "\n",
      "tensor([[2., 1.],\n",
      "        [2., 2.]])\n",
      "0\n",
      "0.0\n",
      "[False]\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "0\n",
      "1.0\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_random = np.mean([np.sum(p['rewards']) for p in paths])\n",
    "print(\"Expected reward (per traj):\", expected_random)\n",
    "expected_random = np.hstack([p['rewards'] for p in paths]).mean()\n",
    "print(\"Expected reward (per step):\", expected_random, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print(s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd99968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward (per traj): 1.0\n",
      "Expected reward (per step): 0.3436426116838488 \n",
      "\n",
      "tensor([[0., 2.],\n",
      "        [1., 2.]])\n",
      "2\n",
      "0.0\n",
      "[False]\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.]])\n",
      "1\n",
      "1.0\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "example_policy = sysRolloutPolicy() \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(100, max_len, False)\n",
    "expected_heuristic = np.mean([np.sum(p['rewards']) for p in paths])\n",
    "print(\"Expected reward (per traj):\", expected_heuristic)\n",
    "expected_heuristic = np.hstack([p['rewards'] for p in paths]).mean()\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print(s.x)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d6d6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5b4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy) \n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(10000, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=1E-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ad6b4",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f57327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1  -> loss:  0.032106838189065456 , rewards: (train)  0.07890625 , (test)  0.030332681017612523\n",
      "iter  2  -> loss:  0.03021396044641733 , rewards: (train)  0.0765625 , (test)  0.02547169811320755\n",
      "iter  3  -> loss:  0.030439704842865466 , rewards: (train)  0.0796875 , (test)  0.02310536044362292\n",
      "iter  4  -> loss:  0.028555273078382015 , rewards: (train)  0.0734375 , (test)  0.022222222222222223\n",
      "iter  5  -> loss:  0.02863702867180109 , rewards: (train)  0.08203125 , (test)  0.02912621359223301\n",
      "iter  6  -> loss:  0.03140421435236931 , rewards: (train)  0.0953125 , (test)  0.024459078080903106\n",
      "iter  7  -> loss:  0.02897170726209879 , rewards: (train)  0.0921875 , (test)  0.029069767441860465\n",
      "iter  8  -> loss:  0.02644291762262583 , rewards: (train)  0.08671875 , (test)  0.0323846908734053\n",
      "iter  9  -> loss:  0.025334426760673524 , rewards: (train)  0.08046875 , (test)  0.03929679420889348\n",
      "iter  10  -> loss:  0.02578802816569805 , rewards: (train)  0.08515625 , (test)  0.02669208770257388\n",
      "iter  11  -> loss:  0.019610949605703355 , rewards: (train)  0.071875 , (test)  0.02926829268292683\n",
      "iter  12  -> loss:  0.025361264776438474 , rewards: (train)  0.0859375 , (test)  0.026794258373205742\n",
      "iter  13  -> loss:  0.019201553799211978 , rewards: (train)  0.06328125 , (test)  0.026897214217098942\n",
      "iter  14  -> loss:  0.02312063695862889 , rewards: (train)  0.08125 , (test)  0.024459078080903106\n",
      "iter  15  -> loss:  0.024629460461437703 , rewards: (train)  0.09140625 , (test)  0.02892960462873674\n",
      "iter  16  -> loss:  0.018776536732912064 , rewards: (train)  0.08828125 , (test)  0.02581261950286807\n",
      "iter  17  -> loss:  0.020825910940766336 , rewards: (train)  0.0796875 , (test)  0.02010968921389397\n",
      "iter  18  -> loss:  0.022975992038846017 , rewards: (train)  0.08046875 , (test)  0.036734693877551024\n",
      "iter  19  -> loss:  0.02098184498026967 , rewards: (train)  0.08359375 , (test)  0.021120293847566574\n",
      "iter  20  -> loss:  0.025051986798644066 , rewards: (train)  0.09140625 , (test)  0.032673267326732675\n",
      "iter  21  -> loss:  0.02599013354629278 , rewards: (train)  0.096875 , (test)  0.045112781954887216\n",
      "iter  22  -> loss:  0.02237735679373145 , rewards: (train)  0.0875 , (test)  0.044967880085653104\n",
      "iter  23  -> loss:  0.023119884729385375 , rewards: (train)  0.09140625 , (test)  0.045064377682403435\n",
      "iter  24  -> loss:  0.027487155236303806 , rewards: (train)  0.10234375 , (test)  0.05263157894736842\n",
      "iter  25  -> loss:  0.027458851970732213 , rewards: (train)  0.1171875 , (test)  0.03389830508474576\n",
      "iter  26  -> loss:  0.02540990635752678 , rewards: (train)  0.0953125 , (test)  0.024390243902439025\n",
      "iter  27  -> loss:  0.02901819422841072 , rewards: (train)  0.103125 , (test)  0.026717557251908396\n",
      "iter  28  -> loss:  0.027672914136201144 , rewards: (train)  0.1 , (test)  0.02432179607109448\n",
      "iter  29  -> loss:  0.027776963263750076 , rewards: (train)  0.10703125 , (test)  0.03560528992878942\n",
      "iter  30  -> loss:  0.03125424813479185 , rewards: (train)  0.1171875 , (test)  0.040625\n",
      "iter  31  -> loss:  0.02970213033258915 , rewards: (train)  0.11796875 , (test)  0.04338624338624339\n",
      "iter  32  -> loss:  0.03290047626942396 , rewards: (train)  0.13671875 , (test)  0.052338530066815145\n",
      "iter  33  -> loss:  0.034106981195509437 , rewards: (train)  0.13125 , (test)  0.03802672147995889\n",
      "iter  34  -> loss:  0.030562363937497138 , rewards: (train)  0.13359375 , (test)  0.030067895247332686\n",
      "iter  35  -> loss:  0.0293699624016881 , rewards: (train)  0.1078125 , (test)  0.03966597077244259\n",
      "iter  36  -> loss:  0.032761172577738765 , rewards: (train)  0.11640625 , (test)  0.02783109404990403\n",
      "iter  37  -> loss:  0.030947476997971533 , rewards: (train)  0.11015625 , (test)  0.026871401151631478\n",
      "iter  38  -> loss:  0.03113595684990287 , rewards: (train)  0.13125 , (test)  0.019945602901178604\n",
      "iter  39  -> loss:  0.03169624637812376 , rewards: (train)  0.1265625 , (test)  0.035317860746720484\n",
      "iter  40  -> loss:  0.03221935667097568 , rewards: (train)  0.12265625 , (test)  0.03273809523809524\n",
      "iter  41  -> loss:  0.03073255904018879 , rewards: (train)  0.12109375 , (test)  0.030362389813907934\n",
      "iter  42  -> loss:  0.027748968079686166 , rewards: (train)  0.10390625 , (test)  0.027884615384615386\n",
      "iter  43  -> loss:  0.0285567257553339 , rewards: (train)  0.096875 , (test)  0.02544769085768143\n",
      "iter  44  -> loss:  0.02956215776503086 , rewards: (train)  0.09921875 , (test)  0.04096638655462185\n",
      "iter  45  -> loss:  0.025929134897887708 , rewards: (train)  0.10703125 , (test)  0.038974358974358976\n",
      "iter  46  -> loss:  0.027718661352992058 , rewards: (train)  0.10390625 , (test)  0.03403403403403404\n",
      "iter  47  -> loss:  0.028240713104605675 , rewards: (train)  0.11953125 , (test)  0.04516129032258064\n",
      "iter  48  -> loss:  0.026827374193817376 , rewards: (train)  0.11171875 , (test)  0.05442176870748299\n",
      "iter  49  -> loss:  0.025171003863215448 , rewards: (train)  0.1203125 , (test)  0.056\n",
      "iter  50  -> loss:  0.02816968522965908 , rewards: (train)  0.11953125 , (test)  0.052222222222222225\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 50\n",
    "n_iter = 10\n",
    "batch_size = 128\n",
    "n_samples = 256\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(batch_size, max_len, False)\n",
    "    avg_r_test.append(np.hstack([p['rewards'] for p in paths]).mean())\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    qf.train(True)    \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "        rewards = torch.Tensor(batch['rewards']).unsqueeze(-1)\n",
    "        terminals = torch.Tensor(batch['terminals'])\n",
    "        actions = torch.Tensor(batch['actions'])\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "        \n",
    "        out = torch.stack(list(map(target_qf, next_obs)), axis=0).cpu()        \n",
    "        target_q_values = out.max(-1, keepdims=True).values\n",
    "        y_target = rewards + (1. - terminals) * 0.95 * target_q_values\n",
    "        out = torch.stack(list(map(qf, obs)), axis=0).cpu()\n",
    "               \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=True)\n",
    "        qf_loss = qf_criterion(y_pred, y_target)\n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "#         pdb.set_trace()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764860c0",
   "metadata": {},
   "source": [
    "```python \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random]*(n_iter*n_epoch), label = \"random\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "# plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1, step=n_iter), avg_r_test, label = \"avg R (test)\") \n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('training_log.png', dpi=300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47b559",
   "metadata": {},
   "source": [
    "#### options ot make it better\n",
    "1. curriculum learning\n",
    "2. higher eps\n",
    "3. normalize input\n",
    "4. better exploration strategy\n",
    "5. one hot encode input (does not scale well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb522b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
