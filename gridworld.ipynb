{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ccace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice, randint, rand, uniform\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.random import erdos_renyi_graph\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, to_undirected\n",
    "\n",
    "import networkx as nx\n",
    "import gym\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy, deepcopy\n",
    "from typing import Optional\n",
    "from enum import Enum, IntEnum\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit\n",
    "from path_collector import MdpPathCollector\n",
    "from any_replay_buffer import anyReplayBuffer\n",
    "# from policies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7314483",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysconfig = namedtuple(\"sysconfig\", \n",
    "                       ['arange', 'maxX', 'maxY', 'goal_reward'], \n",
    "                       defaults=[1., 10., 10., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd803fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class action(IntEnum):\n",
    "    up = 0\n",
    "    down = 1\n",
    "    right = 2\n",
    "    left = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2vecmap = {i: v for i, v in enumerate(torch.Tensor([[0., 1.],\n",
    "                                                     [0, -1.],\n",
    "                                                     [1., 0.], \n",
    "                                                     [-1., 0]]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class droneDeliveryProbe(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    \n",
    "    ### Action Space\n",
    "    Each agent in the scene can move R or L or not at all.\n",
    "    \n",
    "    ### State Space\n",
    "    The state is defined as an arbitrary input array of positions of n drones, appended by the location\n",
    "    of the goal region.\n",
    "    \n",
    "    ### Rewards\n",
    "    The reward of +1 is given to the system for each drone reaching the goal region.\n",
    "    \n",
    "    ### Starting State\n",
    "    Randomly initilized input array.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    When all drones have reached the goal region.\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = sysconfig()\n",
    "        \n",
    "        self.aspace = MultiDiscrete(len(action))\n",
    "        self.sspace = MultiDiscrete([self.config.maxX, self.config.maxY]) # dependency on arange\n",
    "        self.state = None\n",
    "        \n",
    "    def get_distances(self):\n",
    "        return self.state.norm(p=1, keepdim=True)#, dim=1)\n",
    "#         return torch.cdist(X, S[self.n_drones:, :2], p=1)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return torch.Tensor([self.config.maxX, self.config.maxY])\n",
    "        \n",
    "    def reward(self, a):\n",
    "        dis = self.get_distances()        \n",
    "        return (dis==0).sum() * self.config.goal_reward \n",
    "                        \n",
    "    def step(self, a):\n",
    "        err_msg = f\"{a!r} ({type(a)}) is not a valid action.\"\n",
    "        assert self.aspace.contains(a), err_msg\n",
    "                \n",
    "        a = a2vecmap[a]\n",
    "        reward = self.reward(a)\n",
    "        done = all(self.get_distances() == 0)\n",
    "        \n",
    "        self.state = (self.state+a).clamp(min=torch.zeros(2), max=self.get_size()-1)        \n",
    "        return deepcopy(self.state), deepcopy(reward.item()), deepcopy(done), {}\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "            \n",
    "        self.state = torch.Tensor(self.sspace.sample())\n",
    "        return deepcopy(self.state)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self, n: int):\n",
    "        super().reset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = droneDeliveryProbe()\n",
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64c931",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ea600",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ee89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class droneDeliveryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, c_hidden=64, dp_rate_linear=0.5, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(c_in, c_hidden)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(c_hidden, c_out)),\n",
    "            ('act', nn.Softmax()) \n",
    "        ]))\n",
    "\n",
    "        self._device = 'cpu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.to(self._device))\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self._device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 2, len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = droneDeliveryModel(in_channels, out_channels, 16)\n",
    "target_qf = droneDeliveryModel(in_channels, out_channels, 16)\n",
    "qf.to(device)\n",
    "target_qf.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0329f2f",
   "metadata": {},
   "source": [
    "### RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "# from policies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3016d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sysRolloutPolicy(nn.Module, Policy):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        prio = obs.numpy()/obs.sum().item() if obs.sum()!=0 else None\n",
    "#         pdb.set_trace()\n",
    "        return np.random.choice([action.left, action.down], p=prio), {}\n",
    "\n",
    "class argmaxDiscretePolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}\n",
    "\n",
    "# redundant code - clean this up\n",
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1, dim=0):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if rand() < self.eps:\n",
    "            return self.aspace.sample(), {}\n",
    "        q_values = self.qf(obs)\n",
    "        return q_values.cpu().detach().numpy().argmax(self.dim), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28133fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_policy = argmaxDiscretePolicy(qf) \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(250, 40, False)\n",
    "expected_random = np.mean([np.sum(p['rewards']) for p in paths])\n",
    "print(\"Expected reward (per traj):\", expected_random)\n",
    "expected_random = np.hstack([p['rewards'] for p in paths]).mean()\n",
    "print(\"Expected reward (per step):\", expected_random, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c053af",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_policy = sysRolloutPolicy() \n",
    "path_collector = MdpPathCollector(env, example_policy)\n",
    "paths = path_collector.collect_new_paths(250, 40, False)\n",
    "expected_heuristic = np.mean([np.sum(p['rewards']) for p in paths])\n",
    "print(\"Expected reward (per traj):\", expected_heuristic)\n",
    "expected_heuristic = np.hstack([p['rewards'] for p in paths]).mean()\n",
    "print(\"Expected reward (per step):\", expected_heuristic, '\\n')\n",
    "\n",
    "idx = np.random.randint(100)\n",
    "for s, a, r, t in zip(paths[idx]['observations'], paths[idx]['actions'], \n",
    "                      paths[idx]['rewards'], paths[idx]['terminals']):\n",
    "    print(s)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf)\n",
    "# expl_policy = sysRolloutPolicy()\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.aspace, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)\n",
    "replay_buffer = anyReplayBuffer(10000, prioritized=True)\n",
    "optimizer = Adam(qf.parameters(), lr=5E-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18209e1f",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "n_iter = 100\n",
    "batch_size = 128*4\n",
    "max_len = 40\n",
    "n_samples = 256*4\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = eval_path_collector.collect_new_paths(batch_size, max_len, False)\n",
    "    avg_r_test.append(np.hstack([p['rewards'] for p in paths]).mean())\n",
    "    \n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    qf.train(True)    \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "        rewards = torch.Tensor(batch['rewards']).unsqueeze(-1)\n",
    "        terminals = torch.Tensor(batch['terminals'])\n",
    "        actions = torch.Tensor(batch['actions'])\n",
    "\n",
    "        obs = batch['observations']\n",
    "        next_obs = batch['next_observations']\n",
    "    \n",
    "        X = torch.stack(next_obs)\n",
    "        out = target_qf(X).cpu()\n",
    "        \n",
    "        target_q_values = out.max(-1, keepdim=True).values\n",
    "        y_target = rewards + (1. - terminals) * 0.95 * target_q_values\n",
    "        \n",
    "        X = torch.stack(obs)\n",
    "        out = qf(X).cpu()\n",
    "        \n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), len(action))\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=True)\n",
    "        qf_loss = qf_criterion(y_pred, y_target)\n",
    "        \n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#     print(qf.state_dict().items())\n",
    "    \n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    print(\"iter \", i+1, \" -> loss: \", np.mean(loss[-n_iter:]),\n",
    "          \", rewards: (train) \", np.mean(avg_r_train[-n_iter:]),\n",
    "          \", (test) \", avg_r_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_iter*n_epoch), [expected_random]*(n_iter*n_epoch), label = \"do nothing\", color='lightgray')\n",
    "plt.plot(np.arange(n_iter*n_epoch), [expected_heuristic]*(n_iter*n_epoch), label = \"move to closest\",  color='darkgray')\n",
    "\n",
    "plt.plot(np.arange(n_iter*n_epoch), avg_r_train, label = \"avg R (train)\")\n",
    "plt.plot(np.arange(n_iter, n_iter*n_epoch+1, step=n_iter), avg_r_test, label = \"avg R (test)\")\n",
    "plt.legend()\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "plt.savefig('training_log.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f83d68",
   "metadata": {},
   "source": [
    "#### options ot make it better\n",
    "1. curriculum learning\n",
    "2. higher eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cedc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
