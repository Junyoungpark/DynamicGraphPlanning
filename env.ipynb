{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43baa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b7a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import *\n",
    "from gnn_model import GraphGNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e87616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import rlkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7977a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_replay_buffer import anyReplayBuffer\n",
    "from policies import *\n",
    "\n",
    "from rlkit.samplers.data_collector import MdpPathCollector\n",
    "# from rlkit.torch.dqn.dqn import DQNTrainer\n",
    "from dqn import DQNTrainer\n",
    "from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes, n_communities = 100, 4\n",
    "env = CovidSEIR(n_nodes, n_communities)\n",
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ba24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = len(NodeState) # feature space\n",
    "out_channels = len(Measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8791725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a778a3",
   "metadata": {},
   "source": [
    "```python \n",
    "format_input = lambda x: F.one_hot(torch.Tensor(np.vectorize(int)(x)).to(torch.int64), \n",
    "                                   num_classes=len(NodeState)).to(torch.float32)\n",
    "\n",
    "# format_data = lambda x: (format_input(x.x).to(torch.float32),\n",
    "#                          x.edge_index,\n",
    "#                          torch.Tensor(x.edge_attr))\n",
    "format_data = lambda x: (format_input(x.x).to(torch.float32).to(device),\n",
    "                         x.edge_index.to(device), \n",
    "                         torch.Tensor(x.edge_attr).to(device))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5255d0",
   "metadata": {},
   "source": [
    "```python \n",
    "model = GraphGNNModel(in_channels, 64, out_channels)\n",
    "# s, edge_index, edge_weight = format_data(x)\n",
    "out = model(*format_data(x), env.community_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fb2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = GraphGNNModel(in_channels, 256, out_channels)\n",
    "target_qf = GraphGNNModel(in_channels, 256, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87693032",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()\n",
    "eval_policy = argmaxDiscretePolicy(qf, env.community_labels)\n",
    "expl_policy = epsilonGreedyPolicy(qf, env.action_space, args=env.community_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff51881",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_path_collector = MdpPathCollector(env, expl_policy)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17052d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = dict(\n",
    "    algorithm=\"DQN\",\n",
    "    version=\"normal\",\n",
    "    layer_size=256,\n",
    "    replay_buffer_size=int(1E6),\n",
    "    algorithm_kwargs=dict(\n",
    "        num_epochs=3000,\n",
    "        num_eval_steps_per_epoch=5000,\n",
    "        num_trains_per_train_loop=1000,\n",
    "        num_expl_steps_per_train_loop=1000,\n",
    "        min_num_steps_before_training=1000,\n",
    "        max_path_length=1000,\n",
    "        batch_size=256,\n",
    "    ),\n",
    "    trainer_kwargs=dict(\n",
    "        discount=0.99,\n",
    "        learning_rate=3E-4,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cceda",
   "metadata": {},
   "source": [
    "```python\n",
    "variant = dict(\n",
    "    algorithm=\"DQN\",\n",
    "    version=\"normal\",\n",
    "    layer_size=256,\n",
    "    replay_buffer_size=int(1E6),\n",
    "    algorithm_kwargs=dict(\n",
    "        num_epochs=300,\n",
    "        num_eval_steps_per_epoch=500,\n",
    "        num_trains_per_train_loop=100,\n",
    "        num_expl_steps_per_train_loop=100,\n",
    "        min_num_steps_before_training=100,\n",
    "        max_path_length=100,\n",
    "        batch_size=256,\n",
    "    ),\n",
    "    trainer_kwargs=dict(\n",
    "        discount=0.99,\n",
    "        learning_rate=3E-4,\n",
    "    ),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79063c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    qf=qf,\n",
    "    target_qf=target_qf,\n",
    "    qf_criterion=qf_criterion,\n",
    "    args=env.community_labels,\n",
    "    **variant['trainer_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45cc87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = anyReplayBuffer(variant['replay_buffer_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46e285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = TorchBatchRLAlgorithm(\n",
    "    trainer=trainer,\n",
    "    exploration_env=env,\n",
    "    evaluation_env=env,\n",
    "    exploration_data_collector=expl_path_collector,\n",
    "    evaluation_data_collector=eval_path_collector,\n",
    "    replay_buffer=replay_buffer,\n",
    "    **variant['algorithm_kwargs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6896f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 43104.99\n",
      "training\n",
      "2022-02-14 16:18:20.057202 PST | Epoch 0 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               0\n",
      "replay_buffer/size               2000\n",
      "trainer/QF Loss                 43105\n",
      "trainer/Y Predictions Mean          1.34584\n",
      "trainer/Y Predictions Std           0.0168273\n",
      "trainer/Y Predictions Max           1.39327\n",
      "trainer/Y Predictions Min           1.29518\n",
      "expl/num steps total             2000\n",
      "expl/num paths total                2\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 201.89\n",
      "expl/Rewards Std                   57.5488\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              201890\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               201890\n",
      "expl/Returns Min               201890\n",
      "expl/Actions Mean                   1.478\n",
      "expl/Actions Std                    0.612794\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           201890\n",
      "eval/num steps total             5000\n",
      "eval/num paths total                5\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 208.019\n",
      "eval/Rewards Std                   54.0486\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                   43\n",
      "eval/Returns Mean              208019\n",
      "eval/Returns Std                  892.291\n",
      "eval/Returns Max               209092\n",
      "eval/Returns Min               206512\n",
      "eval/Actions Mean                   1.51655\n",
      "eval/Actions Std                    0.585599\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    0\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           208019\n",
      "time/data storing (s)               0.000671176\n",
      "time/evaluation sampling (s)      194.046\n",
      "time/exploration sampling (s)      32.8807\n",
      "time/logging (s)                    0.0109035\n",
      "time/saving (s)                     0.000232639\n",
      "time/training (s)                 848.958\n",
      "time/epoch (s)                   1075.9\n",
      "time/total (s)                   1077.58\n",
      "Epoch                               0\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 58437.508\n",
      "training\n",
      "2022-02-14 16:35:44.289930 PST | Epoch 1 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               1\n",
      "replay_buffer/size               3000\n",
      "trainer/QF Loss                 58437.5\n",
      "trainer/Y Predictions Mean          2.73828\n",
      "trainer/Y Predictions Std           1.17171\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           2.14312e-07\n",
      "expl/num steps total             3000\n",
      "expl/num paths total                3\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.752\n",
      "expl/Rewards Std                   38.5174\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              291752\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291752\n",
      "expl/Returns Min               291752\n",
      "expl/Actions Mean                   1.951\n",
      "expl/Actions Std                    0.281245\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291752\n",
      "eval/num steps total            10000\n",
      "eval/num paths total               10\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000721014\n",
      "time/evaluation sampling (s)      161.062\n",
      "time/exploration sampling (s)      37.5061\n",
      "time/logging (s)                    0.0102791\n",
      "time/saving (s)                     0.000119077\n",
      "time/training (s)                 845.639\n",
      "time/epoch (s)                   1044.22\n",
      "time/total (s)                   2121.81\n",
      "Epoch                               1\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 64858.32\n",
      "training\n",
      "2022-02-14 16:52:51.026597 PST | Epoch 2 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               2\n",
      "replay_buffer/size               4000\n",
      "trainer/QF Loss                 64858.3\n",
      "trainer/Y Predictions Mean          2.98047\n",
      "trainer/Y Predictions Std           1.22618\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           4.874e-08\n",
      "expl/num steps total             4000\n",
      "expl/num paths total                4\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.352\n",
      "expl/Rewards Std                   37.8268\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              292352\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292352\n",
      "expl/Returns Min               292352\n",
      "expl/Actions Mean                   1.955\n",
      "expl/Actions Std                    0.266411\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292352\n",
      "eval/num steps total            15000\n",
      "eval/num paths total               15\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000692991\n",
      "time/evaluation sampling (s)      149.488\n",
      "time/exploration sampling (s)      29.992\n",
      "time/logging (s)                    0.0110271\n",
      "time/saving (s)                     0.000122714\n",
      "time/training (s)                 847.231\n",
      "time/epoch (s)                   1026.72\n",
      "time/total (s)                   3148.55\n",
      "Epoch                               2\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 70487.34\n",
      "training\n",
      "2022-02-14 17:10:32.613284 PST | Epoch 3 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               3\n",
      "replay_buffer/size               5000\n",
      "trainer/QF Loss                 70487.3\n",
      "trainer/Y Predictions Mean          3.19922\n",
      "trainer/Y Predictions Std           1.18724\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.0931e-13\n",
      "expl/num steps total             5000\n",
      "expl/num paths total                5\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 293.273\n",
      "expl/Rewards Std                   34.8756\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expl/Returns Mean              293273\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               293273\n",
      "expl/Returns Min               293273\n",
      "expl/Actions Mean                   1.96\n",
      "expl/Actions Std                    0.251794\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           293273\n",
      "eval/num steps total            20000\n",
      "eval/num paths total               20\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000670359\n",
      "time/evaluation sampling (s)      162.278\n",
      "time/exploration sampling (s)      34.5796\n",
      "time/logging (s)                    0.0109591\n",
      "time/saving (s)                     0.000148514\n",
      "time/training (s)                 864.7\n",
      "time/epoch (s)                   1061.57\n",
      "time/total (s)                   4210.13\n",
      "Epoch                               3\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 71320.086\n",
      "training\n",
      "2022-02-14 17:28:08.708378 PST | Epoch 4 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               4\n",
      "replay_buffer/size               6000\n",
      "trainer/QF Loss                 71320.1\n",
      "trainer/Y Predictions Mean          3.26172\n",
      "trainer/Y Predictions Std           1.131\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.6323e-14\n",
      "expl/num steps total             6000\n",
      "expl/num paths total                6\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.542\n",
      "expl/Rewards Std                   39.3623\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   18\n",
      "expl/Returns Mean              291542\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291542\n",
      "expl/Returns Min               291542\n",
      "expl/Actions Mean                   1.949\n",
      "expl/Actions Std                    0.290515\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291542\n",
      "eval/num steps total            25000\n",
      "eval/num paths total               25\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000700754\n",
      "time/evaluation sampling (s)      163.452\n",
      "time/exploration sampling (s)      31.1549\n",
      "time/logging (s)                    0.0105203\n",
      "time/saving (s)                     0.000121678\n",
      "time/training (s)                 861.46\n",
      "time/epoch (s)                   1056.08\n",
      "time/total (s)                   5266.23\n",
      "Epoch                               4\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 74662.61\n",
      "training\n",
      "2022-02-14 17:45:43.327328 PST | Epoch 5 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               5\n",
      "replay_buffer/size               7000\n",
      "trainer/QF Loss                 74662.6\n",
      "trainer/Y Predictions Mean          3.40625\n",
      "trainer/Y Predictions Std           1.04161\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           7.25701e-14\n",
      "expl/num steps total             7000\n",
      "expl/num paths total                7\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 290.239\n",
      "expl/Rewards Std                   43.3086\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              290239\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               290239\n",
      "expl/Returns Min               290239\n",
      "expl/Actions Mean                   1.94075\n",
      "expl/Actions Std                    0.315023\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           290239\n",
      "eval/num steps total            30000\n",
      "eval/num paths total               30\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000728116\n",
      "time/evaluation sampling (s)      162.476\n",
      "time/exploration sampling (s)      29.747\n",
      "time/logging (s)                    0.0103115\n",
      "time/saving (s)                     0.000119769\n",
      "time/training (s)                 862.368\n",
      "time/epoch (s)                   1054.6\n",
      "time/total (s)                   6320.85\n",
      "Epoch                               5\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 75930.35\n",
      "training\n",
      "2022-02-14 18:03:31.942089 PST | Epoch 6 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               6\n",
      "replay_buffer/size               8000\n",
      "trainer/QF Loss                 75930.4\n",
      "trainer/Y Predictions Mean          3.45703\n",
      "trainer/Y Predictions Std           0.995159\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           5.24109e-14\n",
      "expl/num steps total             8000\n",
      "expl/num paths total                8\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.492\n",
      "expl/Rewards Std                   38.9261\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              291492\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291492\n",
      "expl/Returns Min               291492\n",
      "expl/Actions Mean                   1.95\n",
      "expl/Actions Std                    0.281957\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291492\n",
      "eval/num steps total            35000\n",
      "eval/num paths total               35\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000706581\n",
      "time/evaluation sampling (s)      165.479\n",
      "time/exploration sampling (s)      31.8539\n",
      "time/logging (s)                    0.0110178\n",
      "time/saving (s)                     0.000123004\n",
      "time/training (s)                 871.256\n",
      "time/epoch (s)                   1068.6\n",
      "time/total (s)                   7389.46\n",
      "Epoch                               6\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 79195.625\n",
      "training\n",
      "2022-02-14 18:21:17.333604 PST | Epoch 7 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               7\n",
      "replay_buffer/size               9000\n",
      "trainer/QF Loss                 79195.6\n",
      "trainer/Y Predictions Mean          3.57422\n",
      "trainer/Y Predictions Std           0.898327\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.47004e-15\n",
      "expl/num steps total             9000\n",
      "expl/num paths total                9\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.359\n",
      "expl/Rewards Std                   39.1334\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              291359\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291359\n",
      "expl/Returns Min               291359\n",
      "expl/Actions Mean                   1.94825\n",
      "expl/Actions Std                    0.284731\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291359\n",
      "eval/num steps total            40000\n",
      "eval/num paths total               40\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000682357\n",
      "time/evaluation sampling (s)      159.113\n",
      "time/exploration sampling (s)      32.9045\n",
      "time/logging (s)                    0.0112202\n",
      "time/saving (s)                     0.000121976\n",
      "time/training (s)                 873.349\n",
      "time/epoch (s)                   1065.38\n",
      "time/total (s)                   8454.85\n",
      "Epoch                               7\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 79616.055\n",
      "training\n",
      "2022-02-14 18:39:02.745522 PST | Epoch 8 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               8\n",
      "replay_buffer/size              10000\n",
      "trainer/QF Loss                 79616.1\n",
      "trainer/Y Predictions Mean          3.59375\n",
      "trainer/Y Predictions Std           0.918027\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           7.14506e-14\n",
      "expl/num steps total            10000\n",
      "expl/num paths total               10\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.703\n",
      "expl/Rewards Std                   38.5047\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   21\n",
      "expl/Returns Mean              291703\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291703\n",
      "expl/Returns Min               291703\n",
      "expl/Actions Mean                   1.95075\n",
      "expl/Actions Std                    0.279865\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291703\n",
      "eval/num steps total            45000\n",
      "eval/num paths total               45\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000678699\n",
      "time/evaluation sampling (s)      161.136\n",
      "time/exploration sampling (s)      31.6334\n",
      "time/logging (s)                    0.0113087\n",
      "time/saving (s)                     0.000122162\n",
      "time/training (s)                 872.616\n",
      "time/epoch (s)                   1065.4\n",
      "time/total (s)                   9520.26\n",
      "Epoch                               8\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 80257.08\n",
      "training\n",
      "2022-02-14 18:56:58.194342 PST | Epoch 9 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                               9\n",
      "replay_buffer/size              11000\n",
      "trainer/QF Loss                 80257.1\n",
      "trainer/Y Predictions Mean          3.60938\n",
      "trainer/Y Predictions Std           0.916297\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           2.07484e-15\n",
      "expl/num steps total            11000\n",
      "expl/num paths total               11\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 289.115\n",
      "expl/Rewards Std                   45.8356\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              289115\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               289115\n",
      "expl/Returns Min               289115\n",
      "expl/Actions Mean                   1.9345\n",
      "expl/Actions Std                    0.32513\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           289115\n",
      "eval/num steps total            50000\n",
      "eval/num paths total               50\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000677376\n",
      "time/evaluation sampling (s)      169.32\n",
      "time/exploration sampling (s)      33.659\n",
      "time/logging (s)                    0.0106702\n",
      "time/saving (s)                     0.000154884\n",
      "time/training (s)                 872.44\n",
      "time/epoch (s)                   1075.43\n",
      "time/total (s)                  10595.7\n",
      "Epoch                               9\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 81618.664\n",
      "training\n",
      "2022-02-14 19:14:29.368779 PST | Epoch 10 finished\n",
      "-----------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch                              10\n",
      "replay_buffer/size              12000\n",
      "trainer/QF Loss                 81618.7\n",
      "trainer/Y Predictions Mean          3.66016\n",
      "trainer/Y Predictions Std           0.891438\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           4.74268e-15\n",
      "expl/num steps total            12000\n",
      "expl/num paths total               12\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 291.756\n",
      "expl/Rewards Std                   38.7404\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   19\n",
      "expl/Returns Mean              291756\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               291756\n",
      "expl/Returns Min               291756\n",
      "expl/Actions Mean                   1.9505\n",
      "expl/Actions Std                    0.284692\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           291756\n",
      "eval/num steps total            55000\n",
      "eval/num paths total               55\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000679668\n",
      "time/evaluation sampling (s)      172.247\n",
      "time/exploration sampling (s)      31.5923\n",
      "time/logging (s)                    0.010686\n",
      "time/saving (s)                     0.000124401\n",
      "time/training (s)                 847.308\n",
      "time/epoch (s)                   1051.16\n",
      "time/total (s)                  11646.9\n",
      "Epoch                              10\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 82587.414\n",
      "training\n",
      "2022-02-14 19:31:53.794728 PST | Epoch 11 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              11\n",
      "replay_buffer/size              13000\n",
      "trainer/QF Loss                 82587.4\n",
      "trainer/Y Predictions Mean          3.70703\n",
      "trainer/Y Predictions Std           0.788163\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.60964e-15\n",
      "expl/num steps total            13000\n",
      "expl/num paths total               13\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.953\n",
      "expl/Rewards Std                   36.4658\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   21\n",
      "expl/Returns Mean              292953\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292953\n",
      "expl/Returns Min               292953\n",
      "expl/Actions Mean                   1.957\n",
      "expl/Actions Std                    0.265803\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292953\n",
      "eval/num steps total            60000\n",
      "eval/num paths total               60\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000690939\n",
      "time/evaluation sampling (s)      164.549\n",
      "time/exploration sampling (s)      31.3634\n",
      "time/logging (s)                    0.0104829\n",
      "time/saving (s)                     0.000123468\n",
      "time/training (s)                 848.489\n",
      "time/epoch (s)                   1044.41\n",
      "time/total (s)                  12691.3\n",
      "Epoch                              11\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 79559.5\n",
      "training\n",
      "2022-02-14 19:49:05.016689 PST | Epoch 12 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              12\n",
      "replay_buffer/size              14000\n",
      "trainer/QF Loss                 79559.5\n",
      "trainer/Y Predictions Mean          3.59375\n",
      "trainer/Y Predictions Std           0.913762\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.4203e-14\n",
      "expl/num steps total            14000\n",
      "expl/num paths total               14\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.001\n",
      "expl/Rewards Std                   39.1519\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              292001\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292001\n",
      "expl/Returns Min               292001\n",
      "expl/Actions Mean                   1.953\n",
      "expl/Actions Std                    0.272564\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292001\n",
      "eval/num steps total            65000\n",
      "eval/num paths total               65\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000685008\n",
      "time/evaluation sampling (s)      160.108\n",
      "time/exploration sampling (s)      24.8909\n",
      "time/logging (s)                    0.0105638\n",
      "time/saving (s)                     0.000120802\n",
      "time/training (s)                 846.192\n",
      "time/epoch (s)                   1031.2\n",
      "time/total (s)                  13722.5\n",
      "Epoch                              12\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 79309.664\n",
      "training\n",
      "2022-02-14 20:06:31.195331 PST | Epoch 13 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              13\n",
      "replay_buffer/size              15000\n",
      "trainer/QF Loss                 79309.7\n",
      "trainer/Y Predictions Mean          3.56641\n",
      "trainer/Y Predictions Std           0.97402\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           3.98066e-15\n",
      "expl/num steps total            15000\n",
      "expl/num paths total               15\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.248\n",
      "expl/Rewards Std                   39.3955\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              292248\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292248\n",
      "expl/Returns Min               292248\n",
      "expl/Actions Mean                   1.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expl/Actions Std                    0.284949\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292248\n",
      "eval/num steps total            70000\n",
      "eval/num paths total               70\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000678789\n",
      "time/evaluation sampling (s)      165.399\n",
      "time/exploration sampling (s)      33.6663\n",
      "time/logging (s)                    0.0109883\n",
      "time/saving (s)                     0.000121776\n",
      "time/training (s)                 847.085\n",
      "time/epoch (s)                   1046.16\n",
      "time/total (s)                  14768.7\n",
      "Epoch                              13\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 80338.18\n",
      "training\n",
      "2022-02-14 20:23:55.167363 PST | Epoch 14 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              14\n",
      "replay_buffer/size              16000\n",
      "trainer/QF Loss                 80338.2\n",
      "trainer/Y Predictions Mean          3.625\n",
      "trainer/Y Predictions Std           0.910014\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           6.31136e-16\n",
      "expl/num steps total            16000\n",
      "expl/num paths total               16\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.944\n",
      "expl/Rewards Std                   36.072\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   20\n",
      "expl/Returns Mean              292944\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292944\n",
      "expl/Returns Min               292944\n",
      "expl/Actions Mean                   1.957\n",
      "expl/Actions Std                    0.267677\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292944\n",
      "eval/num steps total            75000\n",
      "eval/num paths total               75\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000695637\n",
      "time/evaluation sampling (s)      165.776\n",
      "time/exploration sampling (s)      30.7881\n",
      "time/logging (s)                    0.0125445\n",
      "time/saving (s)                     0.000122741\n",
      "time/training (s)                 847.381\n",
      "time/epoch (s)                   1043.96\n",
      "time/total (s)                  15812.7\n",
      "Epoch                              14\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 80510.63\n",
      "training\n",
      "2022-02-14 20:41:15.494499 PST | Epoch 15 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              15\n",
      "replay_buffer/size              17000\n",
      "trainer/QF Loss                 80510.6\n",
      "trainer/Y Predictions Mean          3.61719\n",
      "trainer/Y Predictions Std           0.948857\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           1.76471e-16\n",
      "expl/num steps total            17000\n",
      "expl/num paths total               17\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 290.406\n",
      "expl/Rewards Std                   42.4022\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   17\n",
      "expl/Returns Mean              290406\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               290406\n",
      "expl/Returns Min               290406\n",
      "expl/Actions Mean                   1.942\n",
      "expl/Actions Std                    0.310058\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           290406\n",
      "eval/num steps total            80000\n",
      "eval/num paths total               80\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000683932\n",
      "time/evaluation sampling (s)      163.167\n",
      "time/exploration sampling (s)      29.7854\n",
      "time/logging (s)                    0.0113566\n",
      "time/saving (s)                     0.000119918\n",
      "time/training (s)                 847.348\n",
      "time/epoch (s)                   1040.31\n",
      "time/total (s)                  16853\n",
      "Epoch                              15\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 81356.78\n",
      "training\n",
      "2022-02-14 20:58:43.419938 PST | Epoch 16 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              16\n",
      "replay_buffer/size              18000\n",
      "trainer/QF Loss                 81356.8\n",
      "trainer/Y Predictions Mean          3.64453\n",
      "trainer/Y Predictions Std           0.919945\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           2.05856e-17\n",
      "expl/num steps total            18000\n",
      "expl/num paths total               18\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.029\n",
      "expl/Rewards Std                   36.7428\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                   21\n",
      "expl/Returns Mean              292029\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292029\n",
      "expl/Returns Min               292029\n",
      "expl/Actions Mean                   1.95175\n",
      "expl/Actions Std                    0.279145\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292029\n",
      "eval/num steps total            85000\n",
      "eval/num paths total               85\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000679171\n",
      "time/evaluation sampling (s)      165.907\n",
      "time/exploration sampling (s)      33.8953\n",
      "time/logging (s)                    0.0114579\n",
      "time/saving (s)                     0.000121868\n",
      "time/training (s)                 848.094\n",
      "time/epoch (s)                   1047.91\n",
      "time/total (s)                  17900.9\n",
      "Epoch                              16\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 80993.17\n",
      "training\n",
      "2022-02-14 21:16:21.360983 PST | Epoch 17 finished\n",
      "-----------------------------  ----------------\n",
      "epoch                              17\n",
      "replay_buffer/size              19000\n",
      "trainer/QF Loss                 80993.2\n",
      "trainer/Y Predictions Mean          3.65234\n",
      "trainer/Y Predictions Std           0.833991\n",
      "trainer/Y Predictions Max           4\n",
      "trainer/Y Predictions Min           6.24706e-17\n",
      "expl/num steps total            19000\n",
      "expl/num paths total               19\n",
      "expl/path length Mean            1000\n",
      "expl/path length Std                0\n",
      "expl/path length Max             1000\n",
      "expl/path length Min             1000\n",
      "expl/Rewards Mean                 292.854\n",
      "expl/Rewards Std                   35.8636\n",
      "expl/Rewards Max                  300\n",
      "expl/Rewards Min                    0\n",
      "expl/Returns Mean              292854\n",
      "expl/Returns Std                    0\n",
      "expl/Returns Max               292854\n",
      "expl/Returns Min               292854\n",
      "expl/Actions Mean                   1.9575\n",
      "expl/Actions Std                    0.26018\n",
      "expl/Actions Max                    2\n",
      "expl/Actions Min                    0\n",
      "expl/Num Paths                      1\n",
      "expl/Average Returns           292854\n",
      "eval/num steps total            90000\n",
      "eval/num paths total               90\n",
      "eval/path length Mean            1000\n",
      "eval/path length Std                0\n",
      "eval/path length Max             1000\n",
      "eval/path length Min             1000\n",
      "eval/Rewards Mean                 300\n",
      "eval/Rewards Std                    0\n",
      "eval/Rewards Max                  300\n",
      "eval/Rewards Min                  300\n",
      "eval/Returns Mean              300000\n",
      "eval/Returns Std                    0\n",
      "eval/Returns Max               300000\n",
      "eval/Returns Min               300000\n",
      "eval/Actions Mean                   2\n",
      "eval/Actions Std                    0\n",
      "eval/Actions Max                    2\n",
      "eval/Actions Min                    2\n",
      "eval/Num Paths                      5\n",
      "eval/Average Returns           300000\n",
      "time/data storing (s)               0.000681992\n",
      "time/evaluation sampling (s)      160.048\n",
      "time/exploration sampling (s)      32.3506\n",
      "time/logging (s)                    0.0115124\n",
      "time/saving (s)                     0.000123554\n",
      "time/training (s)                 865.514\n",
      "time/epoch (s)                   1057.93\n",
      "time/total (s)                  18958.9\n",
      "Epoch                              17\n",
      "-----------------------------  ----------------\n",
      "evaluation sampling\n",
      "exploration sampling\n",
      "data storing\n",
      "qf loss: 84090.46\n"
     ]
    }
   ],
   "source": [
    "algorithm.to(device)\n",
    "algorithm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a70d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
